{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization Demo\n",
    "\n",
    "This notebook demonstrates the text summarization capabilities of the NLP toolkit, including:\n",
    "- Both abstractive and extractive summarization techniques\n",
    "- Using pre-trained transformer models for summarization\n",
    "- Fine-tuning summarization models\n",
    "- Evaluating summaries with ROUGE metrics\n",
    "- Applying summarization to real-world documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup path to allow importing from the src directory\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import toolkit modules\n",
    "from src.data.preprocessing import TextPreprocessor\n",
    "from src.data.data_loader import get_summarization_loader\n",
    "from src.models.summarizer import TextSummarizer, ExtractiveSummarizer\n",
    "from src.training.metrics import rouge_metrics\n",
    "\n",
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration\n",
    "TASK = \"summarization\"\n",
    "# Models for abstractive and extractive summarization\n",
    "ABSTRACTIVE_MODEL = \"facebook/bart-large-cnn\"\n",
    "DATASET_NAME = \"cnn_dailymail\"\n",
    "DATASET_VERSION = \"3.0.0\"\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_OUTPUT_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 1  # Using just 1 epoch for demonstration purposes\n",
    "\n",
    "# Output directory for model and results\n",
    "OUTPUT_DIR = os.path.join(project_root, \"models\", \"demo_summarizer\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring Extractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample long text for summarization\n",
    "sample_text = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by humans and animals. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\n",
    "\n",
    "The term \"artificial intelligence\" had previously been used to describe machines that mimic and display \"human\" cognitive skills that are associated with the human mind, such as \"learning\" and \"problem-solving\". This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.\n",
    "\n",
    "AI applications include advanced web search engines, recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (such as Tesla), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).\n",
    "\n",
    "As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.\n",
    "\n",
    "Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success and renewed funding. AI research has tried and discarded many different approaches during its lifetime, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical-statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.\n",
    "\n",
    "The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals. To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize extractive summarizers with different methods\n",
    "textrank_summarizer = ExtractiveSummarizer(method=\"textrank\")\n",
    "lsa_summarizer = ExtractiveSummarizer(method=\"lsa\")\n",
    "luhn_summarizer = ExtractiveSummarizer(method=\"luhn\")\n",
    "\n",
    "# Generate summaries\n",
    "textrank_summary = textrank_summarizer.summarize(sample_text, ratio=0.3)\n",
    "lsa_summary = lsa_summarizer.summarize(sample_text, ratio=0.3)\n",
    "luhn_summary = luhn_summarizer.summarize(sample_text, ratio=0.3)\n",
    "\n",
    "# Display summaries\n",
    "print(\"Original Text Length:\", len(sample_text.split()))\n",
    "print(\"\\nTextRank Summary:\")\n",
    "print(textrank_summary)\n",
    "print(\"Length:\", len(textrank_summary.split()))\n",
    "\n",
    "print(\"\\nLSA Summary:\")\n",
    "print(lsa_summary)\n",
    "print(\"Length:\", len(lsa_summary.split()))\n",
    "\n",
    "print(\"\\nLuhn Summary:\")\n",
    "print(luhn_summary)\n",
    "print(\"Length:\", len(luhn_summary.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare extractive methods with a visualization\n",
    "def highlight_common_sentences(original_text, summaries, summary_names):\n",
    "    \"\"\"Highlight sentences in original text based on which extractive methods selected them.\"\"\"\n",
    "    # Split text into sentences\n",
    "    import re\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', original_text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    # Create a dataframe with sentences and which methods included them\n",
    "    data = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        row = {\"id\": i, \"sentence\": sentence}\n",
    "        for name, summary in zip(summary_names, summaries):\n",
    "            # Check if sentence is in summary (approximately)\n",
    "            row[name] = any(sentence in s for s in re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', summary))\n",
    "        data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create HTML with highlighted sentences\n",
    "    html = '<style>'\n",
    "    for i, name in enumerate(summary_names):\n",
    "        hue = i * (360 // len(summary_names))\n",
    "        html += f'.{name} {{ background-color: hsla({hue}, 100%, 90%, 0.5); }}\\n'\n",
    "    html += '</style>'\n",
    "    \n",
    "    html += '<div style=\"line-height: 1.5\">'\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        classes = [name for name in summary_names if row[name]]\n",
    "        \n",
    "        if classes:\n",
    "            class_str = ' '.join(classes)\n",
    "            methods_str = ', '.join(classes)\n",
    "            html += f'<span class=\"{class_str}\" title=\"Methods: {methods_str}\">{sentence}</span> '\n",
    "        else:\n",
    "            html += f'{sentence} '\n",
    "    \n",
    "    html += '</div>'\n",
    "    \n",
    "    # Create a legend\n",
    "    html += '<div style=\"margin-top: 20px\">Legend: '\n",
    "    for name in summary_names:\n",
    "        html += f'<span class=\"{name}\" style=\"padding: 2px 5px; margin-right: 10px;\">{name}</span>'\n",
    "    html += '</div>'\n",
    "    \n",
    "    return html\n",
    "\n",
    "# Create and display the visualization\n",
    "summaries = [textrank_summary, lsa_summary, luhn_summary]\n",
    "summary_names = [\"TextRank\", \"LSA\", \"Luhn\"]\n",
    "html_output = highlight_common_sentences(sample_text, summaries, summary_names)\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize a pre-trained abstractive summarizer\n",
    "abstractive_summarizer = TextSummarizer(model_name=ABSTRACTIVE_MODEL)\n",
    "\n",
    "# Print model information\n",
    "print(f\"Model: {ABSTRACTIVE_MODEL}\")\n",
    "print(f\"Number of parameters: {abstractive_summarizer.get_model_size():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate abstractive summary\n",
    "tokenizer = AutoTokenizer.from_pretrained(ABSTRACTIVE_MODEL)\n",
    "abstractive_summary = abstractive_summarizer.summarize_text(\n",
    "    texts=[sample_text],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=150,\n",
    "    min_length=50,\n",
    "    do_sample=False\n",
    ")[0]\n",
    "\n",
    "print(\"Abstractive Summary:\")\n",
    "print(abstractive_summary)\n",
    "print(\"Length:\", len(abstractive_summary.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare all summaries\n",
    "print(\"Comparison of All Summaries:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Original Text: {len(sample_text.split())} words\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"TextRank Summary: {len(textrank_summary.split())} words\")\n",
    "print(f\"LSA Summary: {len(lsa_summary.split())} words\")\n",
    "print(f\"Luhn Summary: {len(luhn_summary.split())} words\")\n",
    "print(f\"Abstractive Summary: {len(abstractive_summary.split())} words\")\n",
    "\n",
    "# Calculate ROUGE scores between abstractive and extractive summaries\n",
    "# This gives us a measure of how similar these approaches are\n",
    "from src.training.metrics import rouge_metrics\n",
    "\n",
    "extractive_summaries = [textrank_summary, lsa_summary, luhn_summary]\n",
    "for i, summary in enumerate(extractive_summaries):\n",
    "    method = [\"TextRank\", \"LSA\", \"Luhn\"][i]\n",
    "    rouge_scores = rouge_metrics([abstractive_summary], [summary])\n",
    "    print(f\"\\nROUGE scores between Abstractive and {method}:\")\n",
    "    print(f\"  ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {rouge_scores['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize tokenizer and preprocessor\n",
    "tokenizer = AutoTokenizer.from_pretrained(ABSTRACTIVE_MODEL)\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Create dataset loader\n",
    "dataset_loader = get_summarization_loader(\n",
    "    tokenizer=tokenizer,\n",
    "    preprocessor=preprocessor,\n",
    "    max_input_length=MAX_INPUT_LENGTH,\n",
    "    max_output_length=MAX_OUTPUT_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load a small subset of the CNN/DailyMail dataset\n",
    "# We'll limit to a few examples to keep notebook running time reasonable\n",
    "print(\"Loading dataset...\")\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(DATASET_NAME, DATASET_VERSION, split=\"train[:100]\")\n",
    "validation_dataset = load_dataset(DATASET_NAME, DATASET_VERSION, split=\"validation[:50]\")\n",
    "\n",
    "# Combine into expected format\n",
    "combined_dataset = {\n",
    "    \"train\": dataset,\n",
    "    \"validation\": validation_dataset\n",
    "}\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Number of training examples: {len(combined_dataset['train'])}\")\n",
    "print(f\"Number of validation examples: {len(combined_dataset['validation'])}\")\n",
    "\n",
    "# Show example data\n",
    "print(\"\\nExample data:\")\n",
    "example = combined_dataset[\"train\"][0]\n",
    "print(f\"Article: {example['article'][:200]}...\")\n",
    "print(f\"Highlights (Summary): {example['highlights']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess_cnn_dailymail(examples):\n",
    "    \"\"\"Preprocess CNN/DailyMail dataset examples for summarization.\"\"\"\n",
    "    # Convert to expected format\n",
    "    inputs = [doc for doc in examples[\"article\"]]\n",
    "    targets = [summary for summary in examples[\"highlights\"]]\n",
    "    \n",
    "    # Tokenize inputs and targets\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Tokenize targets with special handling\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=MAX_OUTPUT_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Replace pad token id with -100 for loss calculation\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing data...\")\n",
    "tokenized_train_dataset = combined_dataset[\"train\"].map(\n",
    "    preprocess_cnn_dailymail, batched=True, \n",
    "    remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")\n",
    "tokenized_val_dataset = combined_dataset[\"validation\"].map(\n",
    "    preprocess_cnn_dailymail, batched=True, \n",
    "    remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")\n",
    "\n",
    "# Create PyTorch DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    tokenized_val_dataset, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_dataloader)}\")\n",
    "print(f\"Validation batches: {len(val_dataloader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
